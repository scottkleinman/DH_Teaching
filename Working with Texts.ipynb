{
 "metadata": {
  "name": "",
  "signature": "sha256:e8d6546f9c1319b63db7cac0a1243344bcedaa89ffad607de6852d1a93710f23"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Working with Texts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## About this Tutorial\n",
      "This document is a re-writing of [DARIAH-DE's Working With Texts tutorial](https://de.dariah.eu/tatom/working_with_text.html). Basically, it is an attempt explain things for which I think beginners will need more detail and change the emphasis in places. Readers will find it valuable to compare the text and examples here with the original version."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Some Concepts and Terminology\n",
      "A text is a string of characters that can be divided up into tokens. A *tokens* are strings of contiguous characters divided by some kind of boundary, frequently a space or punctuation mark. In English this boundary generally divides the token into words. It is also possible to use string length to divide a text into tokens. For Classical Chinese, for instance, it might be preferable to make every character a separate token. Once a text is divided into tokens, the tokens can be counted. Each countable token is called a *term*.\n",
      "\n",
      "From a processing point view, texts are made up of tokens and terms. A *token* is an identifiable unit in the text. Anything from individual characters, to words, to phrases can be a token. Tokens are sometimes called an *n-grams*, because they can be a strings of any length. Tokens consisting of single words or characters are unigrams, sequences of two words or characters are bi-grams. For literary text analysis word unigrams are typically treated as tokens. If we list the number of times each token is found in the text, we will have a list of words, each of which is different from all the others. These words are called *terms*. For instance, the phrase \"the cat in the hat\" has 5 tokens but 4 terms. We can express this nicely in Python:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The token list\n",
      "tokens = ['the', 'cat', 'in', 'the', 'hat']\n",
      "# Print the length (the number of items) in the tokens list\n",
      "print(len(tokens))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The term counts\n",
      "terms = {\"cat\": 1, \"hat\": 1, \"in\": 1, \"the\": 2}\n",
      "# Print the length (the number of items) in the terms dict\n",
      "print(len(terms))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By using a Python list to represent the tokens, we capture the fact that a text is an ordered list of tokens. A Python dictionary doesn't preserve the original order of items, but for terms, the order doesn't matter (it's alphabetised in the example above). When produce a list of terms and their counts, we essentially turn the text into a \"bag of words\", in which their order of occurrence is lost.\n",
      "\n",
      "It is important to think about the implications of studying bags of words. In the process, relationship *between* words (in terms of grammar and/or proximity) will be lost. One way of addressing this problem is to treat tokens as n-grams. An *n-gram* is a token of variable length. If we use single characters or words as tokens, we are using a type of n-gram called a \"unigram\". But we could also make our tokens sequences of two characters or two words. These are called \"bi-grams\". We could also make the token boundary every third character or word, giving us \"tri-grams\". N-grams can be even bigger, in which case the notation tends to shift towards numbers: 4-grams, 5-grams, etc. Although n-grams can be used to address some of the problems we might identify with using bags of unordered words, a great deal of literary text analysis is doen using word unigrams. This can be very effective, as we'll see from the examples below.\n",
      "\n",
      "A list of term frequencies can also be called a *vector*, borrowing a term from mathematics. Treating texts as vectors of term frequencies enables us to manipulate them using a range of Python functions designed for studying and manipulating vectors. We'll be using two important Python packages for this purpose, `numpy` and `scikit-learn`.\n",
      "\n",
      "Another important concept is the *document-term matrix* (or DTM). *Document* is a generic term for a textual unit, such as a novel, newspaper article, or poem, which we wish to study. It can also mean a segment of some larger unit. For instance, we might divide a novel into ten equal parts, each of which would be a separate document. We can organise our documents into rows, with each term in a separate column, as we would in a spreadsheet. This is the document-term matrix. Part of a DTM might look like this:\n",
      "\n",
      "|       | term1 | term2 | term3 |\n",
      "|-------|------:|------:|------:|\n",
      "| doc1  | 43    |  34   |   56  |\n",
      "| doc2  |  2    |  45   |   90  |\n",
      "| doc3  | 23    |  45   |   78  |\n",
      "\n",
      "The DTM allows us to compare term frequencies in the documents we are studying."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Getting Some Data\n",
      "The DARIAH-DE tutorial provides a data set which can be downloaded by clicking the [datasets.zip](https://de.dariah.eu/tatom/_downloads/datasets.zip) link. Before proceeding, extract the zip file to a place you can remember. We will be using the small collection of novels by Jane Austen and Charlotte Bront\u00eb. Open the `data` folder and locate the `austen-bront\u00eb` folder. Change the `\u00eb` to `e`. Check inside the folder--there may be further examples. Make sure you know the command-line path to the data folder. For instance, if you are on Windows, it might be `C:\\Users\\YOURNAME\\Desktop\\`. For you in a CSUN lab, replace `YOURNAME` with your CSUN ID.\n",
      "\n",
      "The reason we are removing the `\u00eb` is that it is a [Unicode](https://en.wikipedia.org/wiki/Unicode) character, which may be handled differently by different version of Python. If we change it to `e`, we don't have to worry about this complication.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Preparing to Analyze Document Vectors\n",
      "The Python `numpy` package has a special data type that allows us to manipulate a DTM, so we'll begin by importing it (and assign it to the conventional alias alias `np`). We'll also import the `CountVectorizer` functions from `scikit-learn's` text feature extraction suite."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.feature_extraction.text import CountVectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will use `CountVectorizer` both to load the texts and to construct a DTM from them. We'll start by constructing a list of filenames:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filenames = [\n",
      "    'data/austen-bronte/Austen_Emma.txt',\n",
      "    'data/austen-bronte/Austen_Pride.txt',\n",
      "    'data/austen-bronte/Austen_Sense.txt',\n",
      "    'data/austen-bronte/CBronte_Jane.txt',\n",
      "    'data/austen-bronte/CBronte_Professor.txt',\n",
      "    'data/austen-bronte/CBronte_Villette.txt'\n",
      "]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each item in the list is the path to the text file within the data folder you downloaded. You must supply the full path on your disk. For instance, if you are using Windows, it might be `C:\\Users\\YOURNAME\\Desktop\\`. Go ahead and add this to each of the file paths. If you are using Windows, you can change the back slashes to forward slashes. iPython Notebook will still find your files. Do this before continuing. And, remember, each of the items in this list is a string, so it must be fully enclosed in apostrophes quotation marks.\n",
      "\n",
      "Now we'll create a `CountVectorizer` object and tell it that the input will be a file name. We'll assign this to the variable `vectorizer` for short. Next we'll feed each of the files in the `filenames` list into the vectorizer and use the `fit_transform()` function to convert it to a sparse matrix (one in which the count for most terms is 0). We'll assign to a variable called `dtm`. Finally, we'll use the `get_feature_names()` function to get a list of the terms."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(input='filename')\n",
      "dtm = vectorizer.fit_transform(filenames)\n",
      "terms = vectorizer.get_feature_names()\n",
      "\n",
      "dtm = dtm.toarray()  # convert the sparse matrix to a numpy array\n",
      "terms = np.array(terms) # Do the same with the terms list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The last two lines convert our dtm and terms list to numpy arrays, which make it possible to query them for the frequencies of individual terms. Let's try this. Assume that we want to find the frequency of the word \"house\" in *Emma*. This is the 0th text in our filenames list (you can confirm this with `print(filenames[0]))`). We next construct an index for the word \"house\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "house_idx = list(terms).index('house')\n",
      "house_count = dtm[0, house_idx]\n",
      "print(house_count)\n",
      "\n",
      "# An alternative approach\n",
      "house_count = dtm[0, terms == 'house'][0]\n",
      "print(house_count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first method above fetches a list of the terms in our DTM and finds the key, or index, of \"house\" in the list. With that information, we can supply it with the index of *Emma* (0) to find the count for \"house\" in that text. The second code block does the same thing with `numpy's` method of indexing. This returns a list containing first the answer we want and then some other information about the shape of the array. By adding the `[0]` index to the end, we can get Python to print only the first item. You can use which over of these two methods you find most intuitive.\n",
      "\n",
      "Just so we have a sense of what we have just created, here is a section of the document-term matrix for a handful of selected words:\n",
      "\n",
      "|                     |and  |emma |home |house|of  |the |\n",
      "|:--------------------|----:|----:|----:|----:|---:|---:|\n",
      "|Austen_Emma.txt      |4896 |865  |130  |95   |4291|5201|\n",
      "|Austen_Pride.txt     |3584 |0    |66   |107  |3609|4330|\n",
      "|Austen_Sense.txt     |3491 |0    |69   |161  |3572|4105|\n",
      "|CBronte_Jane.txt     |6626 |0    |80   |182  |4364|7846|\n",
      "|CBronte_Professor.txt|2936 |0    |37   |93   |2663|3836|\n",
      "|CBronte_Villette.txt |6374 |0    |121  |129  |4845|8363|"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Comparing Texts\n",
      "Since each row of the document-term matrix is a sequence of a novel's word frequencies, it is possible to put mathematical notions of similarity (or distance) between sequences of numbers in service of calculating the similarity (or distance) between any two novels. One frequently used measure of distance between vectors (a measure easily converted into a measure of similarity) is Euclidean distance. Imagine two document vectors as straight lines starting from the bottom left of a graph. If you draw a straight line connecting the two vectors and measure it, you have the Euclidean distance. Since `scikit-learn` has some built-in functions for calculating Euclidean distances, we'll go ahead and import it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics.pairwise import euclidean_distances\n",
      "dist = euclidean_distances(dtm)\n",
      "np.round(dist, 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we are creating a new distance matrix (also called a \"dissimilarity matrix\") from out DTM. It tells us the distances between each of our document vectors. The calculated results are likely to have lots of decimal places, so the last line just uses a `numpy` function to round the numbers to one decimal places. You can see this in the result.\n",
      "\n",
      "In our list of novels, *Pride and Prejudice* is index 1 and *Jane Eyre* is index 3. We can compare their distances by querying our distance matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Distance between Pride and Prejudice and Jane Eyre\n",
      "dist[1, 3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Try comparing some of the other novels. What can the Euclidean distances tell you about the novels?\n",
      "\n",
      "Another common measure used for comparing texts is Cosine similarity, which works by measuring the angle between the two document vectors. If you want to try this, you can get the code from lines 24-28 of the [original tutorial](https://de.dariah.eu/tatom/working_with_text.html). We will use Euclidean distance for the visualisation and clustering techniques we learn below."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Visualising Document Distances\n",
      "What if we want to compare all the texts at once? If you compared the distances of all the novels, you might have done this, but the implications of the numbers can sometimes be hard to interpret. This is where visualisatio can be useful. If we interpret these distances as x and y coordinates on a graph, we can plot how far apart texts are. A general approach to visualizing distances is to assign a point in a plane to each text, making sure that the distance between points is proportional to the pairwise distances we calculated. This kind of visualization is common enough that it has a name, \"multidimensional scaling (MDS) and family of functions in `scikit-learn`.\n",
      "\n",
      "Before we start, we need to take care of a problem will encounter in creating a graph. Right now, our texts are take the form of long path names. We really want something shorter for use as labels in the graph. So we'll create a list of short labels:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "labels = [\n",
      "    'Austen_Emma',\n",
      "    'Austen_Pride',\n",
      "    'Austen_Sense',\n",
      "    'CBronte_Jane',\n",
      "    'CBronte_Professor',\n",
      "    'CBronte_Villette'\n",
      "]\n",
      "\n",
      "from sklearn.manifold import MDS\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We've also inmported `scikit-learn's` MDS functions and the `maplotlib` plotting library (conventionally) referenced as `plt`. The following two lines will create a matrix of positions. A few explanations are given in comments, but don't worry too much about the details."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# two components as we're plotting points in a two-dimensional plane\n",
      "# \"precomputed\" because we provide a distance matrix\n",
      "# we will also specify `random_state` so the plot is reproducible.\n",
      "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
      "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
      "xs, ys = pos[:, 0], pos[:, 1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we're ready to plot our graph (which will open in another window). The code below creates a scatter plot. By default, the dots on the graph will be sky blue, but the colour is set to orange if the label contains the string \"Austen\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# color-blind-friendly palette\n",
      "for x, y, label in zip(xs, ys, labels):\n",
      "    color = 'orange' if \"Austen\" in label else 'skyblue'\n",
      "    plt.scatter(x, y, c=color)\n",
      "    plt.text(x, y, label)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check out the [original tutorial](https://de.dariah.eu/tatom/working_with_text.html) for a way to plot the graph in 3D (but note that we have changed some of the variable names here, so you'll have to adjust the code.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Cluster Analysis\n",
      "Clustering texts into discrete groups of similar texts is often a useful exploratory step. For example, a researcher may be wondering if certain textual features partition a collection of texts by author or by genre. Pairwise distances alone do not produce any kind of classification. To put a set of distance measurements to work in classification requires additional assumptions, such as a definition of a group or cluster.\n",
      "\n",
      "The ideas underlying the transition from distances to clusters are, for the most part, common sense. Any clustering of texts should result in texts that are closer to each other (in the distance matrix) residing in the same cluster. There are many ways of satisfying this requirement; there no unique clustering based on distances that is the \"best\". For our example, we will use the average Euclidean distance between vectors to produce hierarchical groups of clusters. The algorithm for hierarchical clustering of texts is as follows:\n",
      "\n",
      "1. Start with each document in its own cluster.\n",
      "2. Until only a single cluster remains,\n",
      "   * Find the closest clusters and merge them. The distance between two clusters is the average of the nearest and furthest points in the vector.\n",
      "   * Return a tree containing a record of cluster-merges.\n",
      "\n",
      "Hierarchical clusterings can be visualised with a tree diagram called a dendrogram. In the code below, we use `scikit-learn's` hierarchical clustering functions to do the clustering."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.cluster.hierarchy import dendrogram\n",
      "\n",
      "linkage_matrix = average(dist)\n",
      "dendrogram(linkage_matrix, orientation=\"right\", labels=labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You should see a dictionary containing the numbers used to create the dendrogram. Let's go ahead and plot it. Make sure that you have closed the window for any previous graphs you have created."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.tight_layout()  # fixes margins\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Close the graph window, and let's try that one more time. You might find this orientation easier to interpret."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dendrogram(linkage_matrix, orientation=\"top\", labels=labels)\n",
      "plt.tight_layout()  # fixes margins\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can learn how to interpret dendrograms by watching [How to Read a Dendrogram](https://www.youtube.com/watch?v=MX6AUX1b1w0). Alternatively, running the code below will embed the video in this iPython notebook."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import YouTubeVideo\n",
      "YouTubeVideo(\"MX6AUX1b1w0\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "\n",
        "        <iframe\n",
        "            width=\"400\"\n",
        "            height=300\"\n",
        "            src=\"https://www.youtube.com/embed/MX6AUX1b1w0\"\n",
        "            frameborder=\"0\"\n",
        "            allowfullscreen\n",
        "        ></iframe>\n",
        "        "
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "<IPython.lib.display.YouTubeVideo at 0x18d047b8>"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}